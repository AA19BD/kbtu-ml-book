{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0515e99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Types of ML\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "```{image} https://cdn-images-1.medium.com/max/1600/1*Iz7bCLrPTImnBDOOEyE3LA.png\n",
    ":alt: supervised-learning\n",
    ":class: bg-primary mb-1\n",
    ":width: 500px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "Supervised learning is a popular category of machine learning algorithms that involves training a model on labeled data to make predictions or decisions. In this approach, the algorithm learns from a given set of input-output pairs and uses this knowledge to predict the output for new, unseen inputs. The goal is to find a mapping function that generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2042ae6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now put it more mathematically. Denote\n",
    "\n",
    "* training **dataset** $\\mathcal D = \\{(\\boldsymbol x_i, y_i)\\}_{i=1}^N$;\n",
    "* **features** $\\boldsymbol x \\in \\mathcal X$ (usually $\\mathcal X = \\mathbb R^D$);\n",
    "* **targets** (**labels**) $y_i \\in \\mathcal Y$.\n",
    "\n",
    "The goal of the supervised learning is to find a mapping $f\\colon \\mathcal X \\to \\mathcal Y$ which would minimize the **cost** (**loss**) **function** \n",
    "\n",
    "$$\n",
    "\\mathcal L = \\frac 1N \\sum\\limits_{i=1}^N \\ell(y_i, f(\\boldsymbol x_i)).\n",
    "$$\n",
    "\n",
    "Note that the loss $\\ell(y_i, f(\\boldsymbol x_i))$ is calculated separately on each training object $(\\boldsymbol x_i, y_i)$, and then averaged over the whole training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534e65fe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f888d2",
   "metadata": {},
   "source": [
    "The mapping $f_{\\boldsymbol \\theta}\\colon \\mathcal X \\to \\mathcal Y$ is usually taken from some parametric family \n",
    "\n",
    "$$\n",
    "\\mathcal F = \\{f_{\\boldsymbol \\theta}(\\boldsymbol x) \\vert \\boldsymbol \\theta \\in \\mathbb R^n\\}\n",
    "$$\n",
    "\n",
    "which is also called a **model**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937e856d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To **fit** a model means to find $\\boldsymbol \\theta$ which minimizes the loss function\n",
    "\n",
    "$$\n",
    "    \\mathcal L(\\boldsymbol \\theta) = \\frac 1N \\sum\\limits_{i=1}^N \\ell(y_i, f_{\\boldsymbol \\theta}(\\boldsymbol x_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179b754d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f7dc16",
   "metadata": {},
   "source": [
    "```{image} https://miro.medium.com/max/1400/1*biZq-ihFzq1I6Ssjz7UtdA.jpeg\n",
    ":alt: cats-vs-dogs\n",
    ":class: bg-primary mb-1\n",
    ":width: 500px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096e4748",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Binary classification**\n",
    "\n",
    "* $\\mathcal Y = \\{0, 1\\}$ or $\\mathcal Y = \\{-1, +1\\}$\n",
    "* typical loss function is **misclassification rate**\n",
    "\n",
    "    $$\n",
    "        \\mathcal L(\\boldsymbol \\theta) = \\frac 1N \\sum\\limits_{i=1}^N \\big[y_i \\ne f_{\\boldsymbol \\theta}(\\boldsymbol x_i)\\big]\n",
    "    $$\n",
    "\n",
    "* this loss is not a smooth function, that's why they often predict $\\hat y_i = f_{\\boldsymbol \\theta}(\\boldsymbol x_i)$ which is treated as probability of class $1$, and then use **cross-entropy loss**\n",
    "\n",
    "$$\n",
    "\\mathcal L(\\boldsymbol \\theta) = -\\frac 1N \\sum\\limits_{i=1}^N \\big(y_i \\log(\\hat y_i) + (1-y_i) \\log(1 - \\hat y_i)\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df26db24",
   "metadata": {},
   "source": [
    "```{image} https://miro.medium.com/max/1400/1*JAXmOAImcf683aXaBDPPVg.jpeg\n",
    ":alt: multiclass\n",
    ":class: bg-primary mb-1\n",
    ":width: 500px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593cad82",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Multiclass classification**\n",
    "* $\\mathcal Y = \\{1, 2, \\ldots, K\\}$ \n",
    "* one-hot encoding: $\\boldsymbol y_i \\in \\{0, 1\\}^K$, $\\sum\\limits_{k=1}^K y_{ik} = 1$\n",
    "* $\\hat{\\boldsymbol y}_i = f_{\\boldsymbol \\theta}(\\boldsymbol x_i) \\in [0, 1]^K$ is now the vector of probabilities of belonging to class $k$: \n",
    "\n",
    "    $$\n",
    "        \\hat y_{ik} = \\mathbb P(\\boldsymbol x_i \\in \\text{ class }k)\n",
    "    $$\n",
    "* the cross-entropy loss is now written as follows:\n",
    "\n",
    "$$\n",
    "\\mathcal L(\\boldsymbol \\theta) = -\\frac 1N \\sum\\limits_{i=1}^N \\sum\\limits_{k=1}^Ky_{ik} \\log(\\hat y_{ik})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7344a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad99c8",
   "metadata": {},
   "source": [
    "* $\\mathcal Y = \\mathbb R$ or $\\mathcal Y = \\mathbb R^n$\n",
    "* the common choice is the quadratic loss \n",
    "\n",
    "    $$\n",
    "        \\ell_2(y, \\hat y) = (y - \\hat y)^2\n",
    "    $$\n",
    "* then the overall loss function â€” mean squared error:\n",
    "\n",
    "    $$\n",
    "    \\mathcal L(\\boldsymbol \\theta) = \\mathrm{MSE}(\\boldsymbol \\theta) = \\frac 1N\\sum\\limits_{i=1}^N (y_i - f_{\\boldsymbol \\theta}(\\boldsymbol x_i))^2\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4453d8",
   "metadata": {},
   "source": [
    "If the function $f_{\\boldsymbol \\theta}(\\boldsymbol x_i) = \\boldsymbol {\\theta^\\top x}_i + b$ is linear, then the model is called **linear regression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d427b0a3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.path import Path\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import rc\n",
    "from scipy.special import expit\n",
    "\n",
    "rc('text', usetex=True)\n",
    "rc('text.latex', preamble=r'\\usepackage[utf8]{inputenc}')\n",
    "rc('text.latex', preamble=r'\\usepackage[russian]{babel}')\n",
    "\n",
    "font = {'family' : 'monospace',\n",
    "        'size'   : 24,\n",
    "        'weight' : 'heavy'\n",
    "       }\n",
    "\n",
    "rc('font', **font)\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "def plot_sigmoid(xmin, xmax, ymin, ymax):\n",
    "    text_size = 24\n",
    "    legend_size = 20\n",
    "    eps=0.2\n",
    "    fig, ax = plt.subplots(figsize=(11, 6))\n",
    "    xs = np.linspace(xmin, xmax, num=500)\n",
    "    \n",
    "    \n",
    "    ax.spines['bottom'].set_position('zero')\n",
    "    ax.spines['left'].set_position('zero')\n",
    "\n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    ax.spines['bottom'].set_linewidth(2)\n",
    "    ax.spines['left'].set_linewidth(2)\n",
    "\n",
    "    ax.text(xmax + eps, -.2, r\"$x$\", size=text_size)\n",
    "    ax.text(0.1, ymax, r\"$y$\", size=text_size)\n",
    "    \n",
    "    arrow_fmt = dict(markersize=6, color='black', clip_on=False)\n",
    "    ax.plot((1), (0), marker='>', transform=ax.get_yaxis_transform(), **arrow_fmt)\n",
    "    ax.plot((0), (1), marker='^', transform=ax.get_xaxis_transform(), **arrow_fmt)\n",
    "    \n",
    "    ax.plot(xs, expit(xs), c='r', lw=3, label= r'$\\sigma(x) = \\frac{1}{1+e^{-x}}$')\n",
    "    # plt.plot(xs, np.maximum(0.2*xs, xs), c='m', lw=3, label= r'$\\mathrm{LReLU}(x)$')\n",
    "    \n",
    "    ax.plot([0, xmax], [1, 1], c='k', ls='--', lw=2)\n",
    "    ax.plot([xmin, 0], [-1, -1], c='k', ls='--', lw=2)\n",
    "    \n",
    "    ax.text(-0.18, 0.05, r\"0\")\n",
    "    \n",
    "    ax.legend(fontsize=legend_size);\n",
    "    ax.grid(ls=':')\n",
    "    ax.set_xlim(xmin-eps, xmax+eps)\n",
    "    ax.set_ylim(ymin - eps/2, ymax+eps/2)\n",
    "    yticks = np.arange(ymin, ymax+1)\n",
    "    xticks = np.arange(xmin, xmax+1)\n",
    "    ax.set_yticks(yticks[yticks != 0]);\n",
    "    ax.set_xticks(xticks[xticks != 0])\n",
    "    ax.set_yticklabels(yticks[yticks != 0], size=legend_size)\n",
    "    ax.set_xticklabels(xticks[xticks != 0], size=legend_size);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a43aff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised learning\n",
    "\n",
    "```{image} https://cdn-images-1.medium.com/max/1440/1*YUl_BcqFPgX49sSb5yrk3A.jpeg\n",
    ":alt: unsupervised-learning\n",
    ":class: bg-primary mb-1\n",
    ":width: 500px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922a8c4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "No targets anymore! The training dataset $\\mathcal D = (\\boldsymbol x_i)_{i=1}^N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ff6cd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Examples of unsupervised learning tasks:\n",
    "* clustering\n",
    "* dimension reduction\n",
    "* discovering latent factors\n",
    "* searching for association rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e92206",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Semisupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9a071",
   "metadata": {},
   "source": [
    "\n",
    "```{image} https://cdn-images-1.medium.com/max/1600/1*0TUC4m6yB7HUuPNO2SXEBw.png\n",
    ":alt: semisupervised-learning\n",
    ":class: bg-primary mb-1\n",
    ":width: 500px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "Semi-supervised learning comes into play when you have a dataset that contains both labeled and unlabeled data. Semi-supervised learning is often used in scenarios where obtaining labeled data is expensive, time-consuming, or otherwise challenging. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb47bc",
   "metadata": {},
   "source": [
    "## Reinforcement learning\n",
    "\n",
    "**Reinforcement learning** is a machine learning paradigm where an agent learns to make sequential decisions by interacting with an environment. It aims to maximize a cumulative reward signal by exploring actions and learning optimal strategies through trial and error."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Ð¡Ð»Ð°Ð¹Ð´-ÑˆÐ¾Ñƒ",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
