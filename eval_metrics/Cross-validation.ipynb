{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90558006",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Cross-validation is a widely used technique in machine learning and statistics for assessing the performance and generalization of a predictive model. \n",
    "\n",
    "It helps to evaluate how well a model will perform on unseen data and provides a more robust estimate of a model's performance than a single train-test split. \n",
    "\n",
    "The basic idea is to split the dataset into multiple subsets, train and test the model on different combinations of these subsets, and then aggregate the results to get a more comprehensive performance evaluation. \n",
    "\n",
    "Common types of cross-validation include k-fold cross-validation and leave-one-out cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8280435f",
   "metadata": {},
   "source": [
    "<h6> 1. K-Fold Cross-Validation. </h6>\n",
    "\n",
    "K-Fold Cross-Validation is a widely used technique in machine learning and statistics for assessing the performance and generalization of predictive models. It helps evaluate how well a model will perform on unseen data and provides a more robust estimate of a model's performance than a single train-test split. \n",
    "\n",
    "Here's a step-by-step explanation of K-Fold Cross-Validation:\n",
    "\n",
    "   1. Data Splitting:\n",
    "        * The dataset is divided into k equally sized \"folds\" or subsets.\n",
    "        * Each fold contains approximately the same number of samples.\n",
    "        * For example, if you have 100 data points and choose k=5, each fold will contain 20 data points.\n",
    "\n",
    "        <br>\n",
    "    \n",
    "   2. Training and Testing:\n",
    "        * The model is trained and tested k times.\n",
    "        * In each iteration, one fold is used as the test set, and the remaining k-1 folds are used as the training set.\n",
    "        * For instance, in the first iteration, Fold 1 is the test set, and Folds 2 to k are the training set. In the second iteration, Fold 2 is the test set, and so on.\n",
    "        \n",
    "        <br>\n",
    "    \n",
    "    \n",
    "   3. Performance Metrics:\n",
    "        * After each iteration, a performance metric (e.g., accuracy, mean squared error) is calculated based on the model's performance on the test fold.\n",
    "        * This gives you k performance scores, one for each fold.\n",
    "    \n",
    "        <br>\n",
    "    \n",
    "   4. Aggregation:\n",
    "        * The k performance metrics obtained from the iterations are typically aggregated to get a single estimate of the model's performance.\n",
    "        * Common aggregation methods include taking the mean or median of the k scores.\n",
    "        * This aggregated score provides a more reliable estimate of the model's performance than a single train-test split.\n",
    "    \n",
    "        <br>\n",
    "    \n",
    "   5. Advantages of K-Fold Cross-Validation:\n",
    "        * It provides a more robust estimate of a model's performance since it evaluates the model on multiple test sets.\n",
    "        * It helps detect issues like overfitting and data-specific patterns that may not be evident in a single train-test split.\n",
    "        * It utilizes the entire dataset for training and testing, which is especially useful when the dataset is limited.\n",
    "    \n",
    "        <br>\n",
    "    \n",
    "   6. Common Values of k:\n",
    "        * Common choices for k in K-Fold Cross-Validation include 5, 10, and sometimes 3 or 20, depending on the dataset size and computational resources. The choice of k can vary based on the specific problem.\n",
    "        * In practice, libraries like scikit-learn in Python provide convenient functions to perform K-Fold Cross-Validation with various machine learning models and evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4e08d9",
   "metadata": {},
   "source": [
    "<h5> Now let's use these metrics:</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13600bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 | Mean Squared Error (MSE): 0.8463\n",
      "Fold 1 | Mean Squared Error (MSE): 0.0929\n",
      "Fold 2 | Mean Squared Error (MSE): 3.3469\n",
      "Fold 3 | Mean Squared Error (MSE): 1.5318\n",
      "Fold 4 | Mean Squared Error (MSE): 1.7142\n",
      "\n",
      "Mean Squared Error (MSE) across 5 folds: 1.5064\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Load your dataset and target variable here\n",
    "X, y = make_regression(n_samples=5, n_features=2, noise=1, random_state=42)\n",
    "\n",
    "# Create a machine learning model (replace with your model)\n",
    "model = LinearRegression()\n",
    "\n",
    "# Specify the number of folds (e.g., 5-fold cross-validation)\n",
    "num_folds = 5\n",
    "\n",
    "# Create a KFold object to control the cross-validation process\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation and specify the scoring metric (e.g., mean squared error)\n",
    "scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# The scores are typically negative, so we take their absolute values and calculate the mean\n",
    "mse_scores = -scores\n",
    "mean_mse = mse_scores.mean()\n",
    "\n",
    "for i, mse in enumerate(mse_scores):\n",
    "    print(f\"Fold {i} | Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    \n",
    "print(f\"\\nMean Squared Error (MSE) across {num_folds} folds: {mean_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a96c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84628596, 0.09293514, 3.34686517, 1.53176163, 1.71417011])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f1ea72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c221487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
