{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac20e5e",
   "metadata": {},
   "source": [
    "# ROC-AUC\n",
    "\n",
    "```{figure} https://upload.wikimedia.org/wikipedia/commons/3/36/Roc-draft-xkcd-style.svg\n",
    ":align: center\n",
    "```\n",
    "\n",
    "ROC-AUC (Receiver Operating Characteristic - Area Under the Curve) is a commonly used evaluation metric for binary classification models. \n",
    "\n",
    "It measures the performance of a classifier in distinguishing between the positive and negative classes and is particularly useful when dealing with imbalanced datasets. \n",
    "\n",
    "The ROC-AUC score quantifies the area under the ROC curve, which plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various classification thresholds.\n",
    "\n",
    "\n",
    "1. True Positive Rate (TPR)\n",
    "    $$\n",
    "    \\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "    $$\n",
    "    \n",
    "2. False Positive Rate (FPR)\n",
    "    $$\n",
    "    \\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n",
    "    $$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1ad4ad",
   "metadata": {},
   "source": [
    "Here's how ROC-AUC works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7f8eb",
   "metadata": {},
   "source": [
    "1. Receiver Operating Characteristic (ROC) Curve:\n",
    "\n",
    "    * The ROC curve is created by plotting the True Positive Rate (TPR) on the y-axis and the False Positive Rate (FPR) on the x-axis at different classification thresholds.\n",
    "    \n",
    "    * The ROC curve visually illustrates the trade-off between TPR and FPR as you adjust the classification threshold.\n",
    "    \n",
    "    <br>\n",
    "    \n",
    "2. Area Under the Curve (AUC):\n",
    "    * The ROC-AUC score quantifies the area under the ROC curve.\n",
    "    * A perfect classifier has an AUC of 1, while a random classifier has an AUC of 0.5.\n",
    "    * An AUC value greater than 0.5 indicates that the model is better than random at distinguishing between the classes.\n",
    "\n",
    "    <br>\n",
    "\n",
    "3. Interpretation:\n",
    "    * A higher ROC-AUC score generally indicates a better classifier.\n",
    "    * The ROC-AUC score is independent of the classification threshold and is useful for comparing different classifiers or models.\n",
    "\n",
    "<div>\n",
    "<img src=\"attachment:image.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311538e1",
   "metadata": {},
   "source": [
    "Here's how to compute the ROC-AUC score using Python and scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d90e24ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# True labels and predicted probabilities for the positive class\n",
    "true_labels = [0, 1, 1, 0, 1, 0, 1, 0, 0, 1]\n",
    "predicted_probabilities = [0.2, 0.8, 0.7, 0.3, 0.9, 0.1, 0.6, 0.4, 0.2, 0.75]\n",
    "\n",
    "# Calculate the ROC-AUC score\n",
    "roc_auc = roc_auc_score(true_labels, predicted_probabilities)\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68822126",
   "metadata": {},
   "source": [
    "One more example!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd69d48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.9142\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Generate a synthetic dataset for demonstration (replace with your data)\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a binary classification model (e.g., Logistic Regression)\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the test data\n",
    "y_probabilities = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_probabilities)\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f55777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
