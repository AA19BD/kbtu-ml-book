{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectors\n",
    "\n",
    "В школе учат, что вектор — это направленный отрезок. Такую геометрическую интерпретацию вектора полезно иметь в виду, однако, в прикладных задачах, связанных с машинным обучением и анализом данных, на вектор смотрят как на упорядоченный набор чисел. Его можно записать двумя способами: в строчку или в столбик. Традиция такова, что по умолчанию вектор считается вектором-столбцом, если не оговорено иное. В этом учебнике мы будем придерживаться следующих конвенций:\n",
    "\n",
    "* векторы обозначаются строчными жирными буквами: $\\boldsymbol x$, $\\boldsymbol y$, $\\boldsymbol u$, $\\boldsymbol v$, $\\boldsymbol a$, $\\boldsymbol b, \\ldots$;\n",
    "* по умолчанию подразумевается, что вектор $\\boldsymbol x$ — это вектор-столбец:\n",
    "\n",
    "$$\n",
    "\\boldsymbol x =  \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{pmatrix};\n",
    "$$\n",
    "\n",
    "* для записи в строчку вектор-столбец $\\boldsymbol x$ надо **транспонировать**: \n",
    "\n",
    "    $$\n",
    "    \\boldsymbol x^\\top = (x_1, \\ldots, x_n);\n",
    "    $$\n",
    "\n",
    "* зачастую направление записи вектора (в строчку или в столбец) не имеет существенного значения, и тогда для экономии места вполне допустима запись $\\boldsymbol x = (x_1, \\ldots, x_n)$.\n",
    "\n",
    "Компоненты вектора $\\boldsymbol x = (x_1, \\ldots, x_n)$ называются его **координатами**. Обычно подразумевается, что компоненты вектора — это действительные числа, и поэтому пространство числовых векторов с $n$ координатами обозначают $\\mathbb R^n$. \n",
    "\n",
    "## Vectors in Python\n",
    "\n",
    "For numeric operations with matrices and vectors in Python there is [NumPy](https://numpy.org/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 7]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vector = np.array([1, 2, 7])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `dtype` specifies the underlying type of the vector's elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(vector.dtype)\n",
    "float_vector = np.array([-0.1, 1.123])\n",
    "print(float_vector.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector operations\n",
    "\n",
    "Над векторами определены две основные операции.\n",
    "\n",
    "1. Сложение векторов: если\n",
    "\n",
    "    $$\n",
    "    \\boldsymbol x =  \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{pmatrix}, \\quad\n",
    "    \\boldsymbol y =  \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix},\n",
    "    \\text{ то }\n",
    "    \\boldsymbol x + \\boldsymbol y =  \\begin{pmatrix} x_1 + y_1 \\\\ \\vdots \\\\ x_n + y_n \\end{pmatrix}.\n",
    "    $$\n",
    "\n",
    "2. Умножение вектора на скаляр (число): если\n",
    "\n",
    "    $$\n",
    "    \\boldsymbol x =  \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{pmatrix},\\quad\n",
    "    \\alpha \\in \\mathbb R,\n",
    "    \\text{ то } \n",
    "    \\alpha \\boldsymbol x =  \\begin{pmatrix} \\alpha x_1 \\\\ \\vdots \\\\ \\alpha x_n \\end{pmatrix}.\n",
    "    $$ \n",
    "\n",
    "**Нулевой вектор** $\\boldsymbol 0$, все координаты которого равны нулю, обладает свойством $\\boldsymbol x + \\boldsymbol 0 = \\boldsymbol x$, $\\boldsymbol x \\in \\mathbb R^n$.\n",
    "\n",
    "Вектор $-\\boldsymbol x = (-1)\\cdot \\boldsymbol x$ называется **противоположным вектором** для вектора $\\boldsymbol x$. Вычитание векторов сводится к добавлению противоположного вектора: $\\boldsymbol y - \\boldsymbol x = \\boldsymbol y + (-\\boldsymbol x)$.\n",
    "\n",
    "Векторы $\\boldsymbol x$ и $\\boldsymbol y$ **коллинеарны**, если $\\boldsymbol y = \\alpha \\boldsymbol x$ при некотором $\\alpha \\in \\mathbb R$. Геометрически это означает, что коллинеарные векторы лежат на одной прямой, проходящей через начало координат.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NumPy all these operations are straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.25 0.5  0.75 1.  ]\n",
      "[1 2 3 4 5]\n",
      "Zero vector: [0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(0, 1, num=5)\n",
    "y = np.arange(1, 6)\n",
    "z = np.zeros(5)\n",
    "print(x)\n",
    "print(y)\n",
    "print(\"Zero vector:\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: [1.   2.25 3.5  4.75 6.  ]\n",
      "Diff: [1.   1.75 2.5  3.25 4.  ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sum:\", x+y)\n",
    "print(\"Diff:\", y-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -2 -3 -4 -5]\n"
     ]
    }
   ],
   "source": [
    "print(-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector norm\n",
    "\n",
    "Под **длиной**, или **нормой**, вектора $\\boldsymbol x = (x_1, \\ldots, x_n)$ чаще всего понимают величину\n",
    "\n",
    "$$\n",
    "    \\Vert \\boldsymbol x \\Vert = \\sqrt{\\sum\\limits_{k=1}^n x_k^2}.\n",
    "$$\n",
    "\n",
    "Это так называемая **евклидова норма** вектора $\\boldsymbol x$, представляющая собой обобщение теоремы Пифагора на случай $\\mathbb R^n$. \n",
    "\n",
    "<!--\n",
    "Вообще нормой вектора $\\boldsymbol x \\in \\mathbb R^n$ называется функция $\\Vert\\cdot \\Vert \\colon \\mathbb R^n \\to [0, +\\infty)$ со следующими свойствами:\n",
    "\n",
    "1. $\\Vert\\boldsymbol x\\Vert \\geqslant 0$, $\\Vert\\boldsymbol x\\Vert = 0 \\iff \\boldsymbol x =\\boldsymbol 0$;\n",
    "\n",
    "2. $\\Vert\\alpha \\boldsymbol x \\Vert = \\vert\\alpha\\vert \\Vert\\boldsymbol x \\Vert$ для всех $\\alpha \\in \\mathbb R$, $\\boldsymbol x \\in \\mathbb R^n$;\n",
    "\n",
    "3. $\\Vert\\boldsymbol x + \\boldsymbol y\\Vert \\leqslant \\Vert\\boldsymbol x \\Vert + \\Vert\\boldsymbol y \\Vert$ для всех $\\boldsymbol x, \\boldsymbol y \\in \\mathbb R^n$ (неравенство треугольника).\n",
    "-->\n",
    "Евклидова норма является частным случаем **$p$-нормы** (или **нормы Минковского**)\n",
    "\n",
    "$$\n",
    "    \\Vert \\boldsymbol x \\Vert_p = \\bigg(\\sum\\limits_{k=1}^n |x_k|^p \\bigg)^\\frac 1p, \\quad p \\geqslant 1.\n",
    "$$\n",
    "\n",
    "<!--\n",
    "Неравенство треугольника для $p$-нормы вытекает из [неравенства Минковского](https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D1%80%D0%B0%D0%B2%D0%B5%D0%BD%D1%81%D1%82%D0%B2%D0%BE_%D0%9C%D0%B8%D0%BD%D0%BA%D0%BE%D0%B2%D1%81%D0%BA%D0%BE%D0%B3%D0%BE#%D0%A7%D0%B0%D1%81%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%BB%D1%83%D1%87%D0%B0%D0%B8). Тут важно, что $p \\geqslant 1$, в противном случае неравенство треугольника не выполняется, и $p$-норма уже больше не норма.\n",
    "-->\n",
    "\n",
    "Норма Минковского превращается в\n",
    "\n",
    "* евклидову норму при $p=2$;\n",
    "\n",
    "* **манхэттенскую** норму $\\Vert \\boldsymbol x \\Vert_1 = \\sum\\limits_{k=1}^n \\vert x_k \\vert$ при $p=1$;\n",
    "\n",
    "* **максимальную** норму $$\\Vert \\boldsymbol x \\Vert_\\infty = \\max \\{\\vert x_1 \\vert, \\ldots, \\vert x_n \\vert\\}$$ при $p\\to\\infty$.\n",
    "\n",
    "**Упражнение**. Пусть $\\boldsymbol x \\in \\mathbb R^n$. Докажите следующие неравенства:\n",
    "\n",
    "1. $\\Vert \\boldsymbol x \\Vert_\\infty \\leqslant \\Vert \\boldsymbol x \\Vert_1 \\leqslant n\\Vert \\boldsymbol x \\Vert_\\infty$;\n",
    "\n",
    "2. $\\Vert \\boldsymbol x \\Vert_\\infty \\leqslant \\Vert \\boldsymbol x \\Vert_2 \\leqslant \\sqrt{n}\\Vert \\boldsymbol x \\Vert_\\infty$;\n",
    "\n",
    "3. $\\Vert \\boldsymbol x \\Vert_2 \\leqslant \\Vert \\boldsymbol x \\Vert_1 \\leqslant \\sqrt{n}\\Vert \\boldsymbol x \\Vert_2$.\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "Соотношения 1 и 2 немедленно следуют из неравенств\n",
    "\n",
    "$$\n",
    "    \\max\\{a_1, \\ldots, a_n\\} \\leqslant \\sum\\limits_{k=1}^n a_k \\leqslant n\\max\\{a_1, \\ldots, a_n\\},\n",
    "$$\n",
    "\n",
    "справедливых для любых неотрицательных чисел $a_1, \\ldots, a_n$. Неравенство $\\Vert \\boldsymbol x \\Vert_2 \\leqslant \\Vert \\boldsymbol x \\Vert_1$ эквивалентно неравенству\n",
    "\n",
    "$$\n",
    "    \\sum\\limits_{k=1}^n x_k^2 \\leqslant \\Big(\\sum\\limits_{k=1}^n \\vert x_k\\vert\\Big)^2 =  \\sum\\limits_{k=1}^n x_k^2 + \\underbrace{2\\sum\\limits_{1\\leqslant i < j\\leqslant n} \\vert x_i\\vert \\vert x_j\\vert}_{\\geqslant 0},\n",
    "$$\n",
    "\n",
    "которое, очевидно, выполнено. Наконец, последнее неравенство вытекает из неравенства \n",
    "Коши—Буняковского—Шварца.\n",
    "\n",
    "$$\n",
    "    \\Vert \\boldsymbol x \\Vert_1 = \\sum\\limits_{k=1}^n 1\\cdot\\vert x_k\\vert \\leqslant \\sqrt{\\sum\\limits_{k=1}^n 1^2}\\sqrt{\\sum\\limits_{k=1}^n \\vert x_k\\vert^2} = \\sqrt n \\Vert \\boldsymbol x \\Vert_2.\n",
    "$$\n",
    "```\n",
    "\n",
    "**Упражнение**. Докажите, что $p$-норма при $1\\leqslant p \\leqslant \\infty$ удовлетворяет следующим свойствам:\n",
    "\n",
    "1. $\\Vert\\boldsymbol x\\Vert \\geqslant 0$, $\\Vert\\boldsymbol x\\Vert = 0 \\iff \\boldsymbol x =\\boldsymbol 0$;\n",
    "\n",
    "2. $\\Vert\\alpha \\boldsymbol x \\Vert = \\vert\\alpha\\vert \\Vert\\boldsymbol x \\Vert$ для всех $\\alpha \\in \\mathbb R$, $\\boldsymbol x \\in \\mathbb R^n$;\n",
    "\n",
    "3. $\\Vert\\boldsymbol x + \\boldsymbol y\\Vert \\leqslant \\Vert\\boldsymbol x \\Vert + \\Vert\\boldsymbol y \\Vert$ для всех $\\boldsymbol x, \\boldsymbol y \\in \\mathbb R^n$ (неравенство треугольника).\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "Первые два свойства легко проверяются напрямую. Свойство 3 для $p=1$ и $p=\\infty$ следует из неравенства $\\vert a+b\\vert \\leqslant \\vert a\\vert + \\vert b \\vert$. Для остальных значений $p$ неравенство треугольника эквивалентно [неравенству Минковского](https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D1%80%D0%B0%D0%B2%D0%B5%D0%BD%D1%81%D1%82%D0%B2%D0%BE_%D0%9C%D0%B8%D0%BD%D0%BA%D0%BE%D0%B2%D1%81%D0%BA%D0%BE%D0%B3%D0%BE#%D0%A7%D0%B0%D1%81%D1%82%D0%BD%D1%8B%D0%B5_%D1%81%D0%BB%D1%83%D1%87%D0%B0%D0%B8){:target=\"_blank\"}.\n",
    "```\n",
    "\n",
    "Три свойства из последнего упражнения являются аксиомами нормы в $\\mathbb R^n$. Иначе говоря, всякая функция $\\Vert\\cdot \\Vert \\colon \\mathbb R^n \\to [0, +\\infty)$, удовлетворяющая этим трём свойством, по определению называется нормой. В частности, $p$-норма является нормой при $1\\leqslant p \\leqslant \\infty$. Отметим, что $p$-норма перестаёт быть нормой при $p<1$, поскольку неравенство треугольника в таком случае не выполняется.\n",
    "\n",
    "Вектор $\\boldsymbol x$ называется **единичным**, если его норма равна единице: $\\Vert \\boldsymbol x\\Vert = 1$. **Единичным шаром** в $\\mathbb R^n$ называется множество\n",
    "\n",
    "$$\n",
    "    \\{\\boldsymbol x \\in \\mathbb R^n\\colon \\Vert \\boldsymbol x \\Vert \\leqslant 1\\}.\n",
    "$$\n",
    "\n",
    "Для случая евклидовой нормы это действительно шар при $n=3$ (круг при $n=2$). Единичные шары для $p$-нормы при различных значениях $p$ изображены на рисунке ниже.\n",
    "\n",
    "\n",
    "Расстояние между векторами $\\boldsymbol x$ и $\\boldsymbol y$ вычисляется по формуле $\\Vert \\boldsymbol x - \\boldsymbol y\\Vert$. Норма здесь может быть любой, наиболее популярный выбор — опять же евклидова норма $\\Vert\\boldsymbol x - \\boldsymbol y\\Vert_2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to calculate norm in NumPy? Use [np.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, -2])\n",
    "np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify $p$ use parameter `ord`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-norm = 5.0\n",
      "2-norm = 3.0\n",
      "10-norm = 2.143651567459133\n",
      "infinite norm = 2.0\n"
     ]
    }
   ],
   "source": [
    "print(\"1-norm =\", np.linalg.norm(x, ord=1))\n",
    "print(\"2-norm =\", np.linalg.norm(x, ord=2))\n",
    "print(\"10-norm =\", np.linalg.norm(x, ord=10))\n",
    "print(\"infinite norm =\", np.linalg.norm(x, ord=np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner product\n",
    "\n",
    "**Скалярным произведением** векторов $\\boldsymbol x, \\boldsymbol y \\in \\mathbb R^n$ называется величина\n",
    "\n",
    "$$\n",
    "    \\langle \\boldsymbol x, \\boldsymbol y \\rangle = \\sum\\limits_{k=1}^n x_k y_k.\\tag{1}\n",
    "$$ \n",
    "\n",
    "Альтернативное обозначение: $\\boldsymbol x^\\top \\boldsymbol y$ (объяснение этого обозначения см. в одной из [следующих секций](#умножение-матрицы-на-вектор)). \n",
    "\n",
    "<!--\n",
    "Скалярное произведение является функцией $\\langle \\cdot, \\cdot \\rangle \\colon \\mathbb R^n\\times \\mathbb R^n \\to \\mathbb R$ со следующими свойствами:\n",
    "\n",
    "1. $\\langle \\boldsymbol x, \\boldsymbol x \\rangle \\geqslant 0$, $\\langle \\boldsymbol x, \\boldsymbol x \\rangle = 0 \\iff \\boldsymbol x = \\boldsymbol 0$ (положительная определённость);\n",
    "\n",
    "2. $\\langle \\boldsymbol x, \\boldsymbol y \\rangle = \\langle \\boldsymbol y, \\boldsymbol x \\rangle$ (симметричность);\n",
    "\n",
    "3. $\\langle \\alpha\\boldsymbol x + \\beta\\boldsymbol y, \\boldsymbol z \\rangle = \\alpha \\langle \\boldsymbol x, \\boldsymbol z \\rangle + \\beta \\langle \\boldsymbol y, \\boldsymbol z \\rangle$ (линейность).\n",
    "\n",
    "Нетрудно проверить, что определённое по формуле (1) стандартное скалярное произведение в $\\mathbb R^n$ удовлетворяет всем этим свойствам. Существуют и другие виды скалярных произведений, но на практике они редко находят применение.\n",
    "\n",
    "\n",
    "Скалярное произведение автоматически порождает норму по формуле $\\Vert \\boldsymbol x \\Vert = \\sqrt{\\langle \\boldsymbol x, \\boldsymbol x \\rangle}$. Первые два свойства такой нормы немедленно вытекают из определения скалярного произведения, а \n",
    "-->\n",
    "\n",
    "Скалярное произведение порождает евклидову норму в $\\mathbb R^n$: \n",
    "\n",
    "$$\n",
    "\\sqrt{\\langle \\boldsymbol x, \\boldsymbol x \\rangle} = \\Vert \\boldsymbol x \\Vert_2.\n",
    "$$\n",
    "\n",
    "**Упражнение**. Докажите неравенство Коши—Буняковского—Шварца\n",
    "\n",
    "$$\n",
    "    \\bigg(\\sum\\limits_{k=1}^n x_k y_k\\bigg)^2 \\leqslant \\sum\\limits_{k=1}^n x_k^2 \\sum\\limits_{k=1}^n y_k^2, \n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary markdown=\"span\">Решение </summary>\n",
    "<div>\n",
    "\n",
    "[Здесь](https://rgmia.org/papers/v12e/Cauchy-Schwarzinequality.pdf){:target=\"_blank\"} можно найти 12 различных доказательств этого замечательного неравенства. Вот первое из них:\n",
    "\n",
    "$$\n",
    "    \\begin{multline*}\n",
    "    0\\leqslant \\sum\\limits_{i, j=1}^n (x_i y_j - x_j y_i)^2 = \\\\\n",
    "    =\\sum\\limits_{i=1}^n x_i^2 \\sum\\limits_{j=1}^n y_j^2 + \\sum\\limits_{i=1}^n y_i^2 \\sum\\limits_{j=1}^n x_j^2 - 2  \\sum\\limits_{i=1}^n x_i y_i\\sum\\limits_{j=1}^n x_j y_j = \\\\\n",
    "    =2\\sum\\limits_{i=1}^n x_i^2 \\sum\\limits_{j=1}^n y_j^2 - 2\\bigg(\\sum\\limits_{i=1}^n x_i y_i\\bigg)^2,\n",
    "    \\end{multline*}\n",
    "$$\n",
    "\n",
    "откуда и вытекает требуемое. Из приведённого рассуждения также следует, что неравенство Коши—Буняковского—Шварца обращается в равенство в следующих случаях:\n",
    "\n",
    "* $x_i = 0$ при всех $i=1, \\ldots, n$;\n",
    "* $y_i = 0$ при всех $i=1, \\ldots, n$;\n",
    "* $\\frac{x_i}{y_i} = C$ при всех $i=1, \\ldots, n$.\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "\n",
    "С учётом введённых выше определений евклидовой нормы и скалярного произведения неравенство Коши—Буняковского—Шварца можно переписать в виде\n",
    "\n",
    "$$\n",
    "    \\vert\\langle \\boldsymbol x, \\boldsymbol y \\rangle\\vert \\leqslant \\Vert \\boldsymbol x \\Vert \\cdot \\Vert \\boldsymbol y \\Vert.\n",
    "$$\n",
    "\n",
    "Это неравенство обращается в равенство тогда и только тогда, когда векторы $\\boldsymbol x$ и $\\boldsymbol y$ коллинеарны.\n",
    "\n",
    "Если скалярное произведение векторов $\\boldsymbol x$ и $\\boldsymbol y$ равно нулю, то они называются **ортогональными**. Угол $\\theta$ между двумя ненулевыми векторами $\\boldsymbol x$ и $\\boldsymbol y$ определяется из соотношения\n",
    "\n",
    "$$\n",
    "    \\cos \\theta = \\frac{\\langle \\boldsymbol x, \\boldsymbol y \\rangle}{\\Vert \\boldsymbol x\\Vert \\cdot \\Vert \\boldsymbol y\\Vert}.\n",
    "$$\n",
    "\n",
    "В силу неравенства Коши—Буняковского—Шварца эта дробь всегда находится в диапазоне от $-1$ до $1$, поэтому такое определение угла между векторами корректно.\n",
    "Если $\\langle \\boldsymbol x, \\boldsymbol y \\rangle = 0$, то $\\cos\\theta = 0$ и $\\theta = \\frac \\pi 2$. Таким образом, угол между ортогональными векторами равен $90°$.\n",
    "\n",
    "В анализе данных косинус угла между векторами часто берут в качестве меры их близости (**cosine similarity**): чем ближе $\\cos\\theta$ к единице, тем более похожими считаются вектора. Похожая метрика **косинусное расстояние** (**cosine distance**) определяется по формуле $1-\\cos\\theta$.\n",
    "\n",
    "Отметим, что для вычисления скалярного произведения по формуле (1) требуется выполнить $n$ умножений и $n-1$ сложение. Если измерять трудоёмкость подсчёта скалярного произведения $n$-мерных векторов в количестве арифметических операций, то она составит $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several way to calculate the inner product of two vectors in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3 3\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "y = np.array([1, -2, 2])\n",
    "print(np.dot(x, y), x.dot(y), x @ y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Матричные операции\n",
    "\n",
    "Перечислим поэлементные операции с матрицами. Пусть матрицы $\\boldsymbol A, \\boldsymbol B \\in \\mathbb R^{m\\times n}$, $\\alpha \\in \\mathbb R$, $1 \\leqslant i \\leqslant m$, $1\\leqslant j \\leqslant n$.\n",
    "\n",
    "* Сложение матриц: \n",
    "\n",
    "$$\n",
    "\\boldsymbol С = \\boldsymbol A + \\boldsymbol B \\iff  C_{ij} = A_{ij} + B_{ij}.\n",
    "$$\n",
    "\n",
    "* Умножение матрицы на скаляр: \n",
    "\n",
    "$$\n",
    "    \\boldsymbol B = \\alpha \\boldsymbol A \\iff B_{ij} = \\alpha A_{ij}.\n",
    "$$\n",
    "\n",
    "* Поэлементное умножение (**произведение Адамара**) и деление матриц:\n",
    "\n",
    "    $$\n",
    "        \\boldsymbol C = \\boldsymbol A \\odot \\boldsymbol B \\iff C_{ij} = A_{ij} B_{ij},\\quad\n",
    "        \\boldsymbol D = \\boldsymbol A \\oslash \\boldsymbol B \\iff D_{ij} = \\frac{A_{ij}}{B_{ij}}\n",
    "    $$\n",
    "\n",
    "    (последнее возможно, разумеется, только если все элементы матрицы $\\boldsymbol B$ не равны нулю). Стоит отметить, что под произведением матриц обычно понимают совсем иную операцию, нежели поэлементное умножение, и о ней предстоит отдельный разговор.\n",
    "\n",
    "Первые два свойства указывают на то, что матрицы можно рассматривать как вектора особого вида. Можно даже пойти дальше и записать матрицу размера $m\\times n$ в виде одномерного массива длины $mn$ (например, построчно), и получится вектор в чистом виде. Такая операция применяется, к примеру, в свёрточных нейронных сетях, когда на определённом этапе представленная матрицей или тензором картинка выпрямляется (flatten) в вектор для дальнейшего прохождения через один или несколько полносвязных слоёв.\n",
    "\n",
    "## Умножение матрицы на вектор\n",
    "\n",
    "Чтобы матрицу $\\boldsymbol A$ можно было умножить на вектор $\\boldsymbol x$, число её столбцов должно совпадать с размерностью вектора $\\boldsymbol x$. Если $\\boldsymbol A \\in \\mathbb R^{m\\times n}$,  $\\boldsymbol x \\in \\mathbb R^n$, то  $\\boldsymbol A\\boldsymbol x =  \\boldsymbol b \\in \\mathbb R^m$; координаты вектора  $\\boldsymbol b$ вычисляются по формуле\n",
    "\n",
    "$$\n",
    "    b_i = \\sum\\limits_{j=1}^n A_{ij} x_j, \\quad 1 \\leqslant i \\leqslant m.\n",
    "$$\n",
    "\n",
    "По-другому можно сказать, что $b_i =  \\boldsymbol a_i^\\top  \\boldsymbol x$, где  $\\boldsymbol a_i^\\top$ — $i$-я строка матрицы  $\\boldsymbol A$. Таким образом, для умножения матрицы на вектор требуется вычислить $m$ скалярных произведений $n$-мерных векторов, поэтому сложность этих вычислений составляет $O(mn)$. Если матрица  $\\boldsymbol A$ квадратная размера $n\\times n$, то эта сложность равна $O(n^2)$.\n",
    "\n",
    "На матрично-векторное умножение можно взглянуть с другой стороны. А именно, если обозначить $j$-й столбец через $\\boldsymbol a_j$, то получается, что\n",
    "\n",
    "$$\n",
    "   \\boldsymbol b =  \\boldsymbol A\\boldsymbol x =  \\sum\\limits_{j=1}^n x_j \\boldsymbol a_j.\n",
    "$$\n",
    "\n",
    "Таким образом, вектор $\\boldsymbol b =  \\boldsymbol A\\boldsymbol x$ является линейной комбинацией столбцов матрицы $\\boldsymbol A$ с координатами вектора $\\boldsymbol x$ в качестве коэффициентов.\n",
    "\n",
    "Умножая матрицу на вектор-столбец, мы получаем снова вектор-столбец; умножение матрицы на вектор-строку записывают в обратном порядке:\n",
    "\n",
    "$$\n",
    "    \\boldsymbol y^\\top \\boldsymbol A = \\boldsymbol c^\\top,\\quad \\boldsymbol y \\in \\mathbb R^m, \\quad \\boldsymbol A \\in \\mathbb R^{m\\times n}, \\quad \\boldsymbol c \\in \\mathbb R^n, \\quad\n",
    "    c_j = \\sum\\limits_{i=1}^m y_i A_{ij}.\n",
    "$$\n",
    "\n",
    "Результат такого умножения представляет собой линейную комбинацию строк матрицы $\\boldsymbol A$:  $\\boldsymbol c^\\top = \\sum\\limits_{i=1}^m y_i \\boldsymbol a_i^\\top$. \n",
    "\n",
    "Заметим, что матрицу $\\boldsymbol A$ размера $1 \\times n$ можно представить в виде вектора-строки $\\boldsymbol a^\\top$. С другой стороны, согласно определению результат её умножения на вектор $\\boldsymbol x \\in \\mathbb R^n$ равен числу \n",
    "\n",
    "$$\n",
    "\\boldsymbol A \\boldsymbol x = \\boldsymbol a^\\top \\boldsymbol x = \\sum\\limits_{j=1}^n a_j x_j,\n",
    "$$\n",
    "\n",
    "а это в точности скалярное произведение $\\langle \\boldsymbol a, \\boldsymbol x\\rangle$. Отсюда и проистекает его весьма популярное обозначение через матрично-векторное произведение $\\boldsymbol a^\\top \\boldsymbol x$, которым мы также будем активно пользоваться в дальнейшем.\n",
    "\n",
    "## Произведение матриц\n",
    "\n",
    "Матрицу $\\boldsymbol A$ можно умножить на матрицу $\\boldsymbol B$, если количество столбцов матрицы $\\boldsymbol A$ равно числу строк матрицы $\\boldsymbol B$. А именно, **произведением** матриц $\\boldsymbol A \\in\\mathbb R^{m\\times n}$ и $\\boldsymbol B \\in\\mathbb R^{n\\times p}$ является матрица $\\boldsymbol C = \\boldsymbol A \\boldsymbol B \\in\\mathbb R^{m\\times p}$, каждый элемент которой равен\n",
    "\n",
    "$$\n",
    "    C_{ik} = \\sum\\limits_{j=1}^n A_{ij}B_{jk},\\quad 1\\leqslant i \\leqslant m, \\quad 1\\leqslant k \\leqslant p.\n",
    "$$\n",
    "\n",
    "Элементы матрицы $\\boldsymbol C$ можно записать в виде $C_{ik} = \\boldsymbol a_i^\\top \\boldsymbol b_k$, где $\\boldsymbol a_i^\\top$ — строки матрицы $\\boldsymbol A$, а $\\boldsymbol b_k$ — столбцы матрицы $\\boldsymbol B$. Вычисление каждого элемента $C_{ik}$ требует $O(n)$ арифметических операций, поэтому общая сложность подсчёта произведения матриц составляет $O(mnp)$. Если все матрицы квадратные размера $n\\times n$, то сложность будет $O(n^3)$.\n",
    "\n",
    "<details>\n",
    " <summary markdown=\"span\">А быстрее можно? </summary>\n",
    "<div>\n",
    "\n",
    "[Алгоритм Штрассена](https://ru.wikipedia.org/wiki/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC_%D0%A8%D1%82%D1%80%D0%B0%D1%81%D1%81%D0%B5%D0%BD%D0%B0){:target=\"_blank\"} позволяет перемножать квадратные матрицы чуть быстрее, а именно за $O(n^{\\log_2 7})$, что примерно составляет $O(n^{2.81})$. Алгоритм Штрассена был разработан ещё в 1969 году, и с тех пор было предложено множество алгоритмов с лучшей асимптотикой. Подробнее про эту гонку можно почитать в этой [статье](https://www.quantamagazine.org/mathematicians-inch-closer-to-matrix-multiplication-goal-20210323/){:target=\"_blank\"}, где текущая SoTA указана как $O(n^{2.3728596})$. Математики не оставляют надежд добиться асимптотики $O(n^2)$ или хотя бы $O(n^{2+\\varepsilon})$ для любого сколь угодно малого $\\varepsilon > 0$, однако, все эти изыскания носят сугубо теоретический характер из-за гигантских констант в O-большом и сложности реализации подобных алгоритмов.\n",
    "\n",
    "Впрочем, сегодня новые алгоритмы перемножения матриц придумывают не только математики, но и нейронные сети.  Например, компания DeepMind с помощью глубокого обучения подкрепления разработала алгоритм [AlphaTensor](https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor){:target=\"_blank\"}, способный перемножать матрицы размера $5\\times 5$ эффективнее, чем алгоритм Штрассена ($76$ умножений против $80$).\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "Произведение матриц $\\boldsymbol A \\boldsymbol B$ можно представить также в виде набора столбцов, получающихся умножением матрицы $\\boldsymbol A$ на столбцы матрицы $\\boldsymbol B$: если $\\boldsymbol B = [\\boldsymbol b_1 \\ldots \\boldsymbol b_p]$, то\n",
    "\n",
    "$$\n",
    "\\boldsymbol A \\boldsymbol B = [\\boldsymbol A\\boldsymbol b_1 \\ldots \\boldsymbol A\\boldsymbol b_p].\n",
    "$$ \n",
    "\n",
    "Или же можно строки матрицы $\\boldsymbol A$ умножать на матрицу $\\boldsymbol B$: если\n",
    "\n",
    "$$\n",
    "    \\boldsymbol A  = \\begin{pmatrix}\n",
    "        \\boldsymbol a_1^\\top \\\\\n",
    "        \\boldsymbol a_2^\\top \\\\\n",
    "        \\vdots\\\\\n",
    "        \\boldsymbol a_m^\\top \\\\\n",
    "    \\end{pmatrix},\n",
    "    \\text{ то }\n",
    "    \\boldsymbol A \\boldsymbol B = \n",
    "    \\begin{pmatrix}\n",
    "        \\boldsymbol a_1^\\top \\boldsymbol B \\\\\n",
    "        \\boldsymbol a_2^\\top \\boldsymbol B \\\\\n",
    "        \\vdots\\\\\n",
    "        \\boldsymbol a_m^\\top \\boldsymbol B \\\\\n",
    "    \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "**Свойства матричного умножения** (предполагается, что размеры всех матриц согласованы)\n",
    "\n",
    "1. $(\\boldsymbol{AB})\\boldsymbol{C} = \\boldsymbol{A}(\\boldsymbol{BC})$ (ассоциативность)\n",
    "\n",
    "2. $(\\boldsymbol A + \\boldsymbol B)\\boldsymbol{C} = \\boldsymbol{AC} + \\boldsymbol{BC}$, $\\boldsymbol{C}(\\boldsymbol A + \\boldsymbol B) = \\boldsymbol{CA} + \\boldsymbol{CB}$ (дистрибутивность)\n",
    "\n",
    "3. $(\\boldsymbol{AB})^\\top = \\boldsymbol B^\\top \\boldsymbol A^\\top$\n",
    "\n",
    "4. $\\mathrm{tr}(\\boldsymbol{AB}) = \\mathrm{tr}(\\boldsymbol{BA})$ \n",
    "\n",
    "Последнее свойство обобщается до равенства $\\mathrm{tr}(\\boldsymbol{ABС}) = \\mathrm{tr}(\\boldsymbol{СAB})$, которое справедливо для любого числа сомножителей с согласованными размерами. Это так называемое **циклическое свойство** следа матрицы.\n",
    "\n",
    "**Упражнение**. Докажите равенства $(\\boldsymbol{AB})^\\top = \\boldsymbol B^\\top \\boldsymbol A^\\top$ и $\\mathrm{tr}(\\boldsymbol{AB}) = \\mathrm{tr}(\\boldsymbol{BA})$ для матриц подходящего размера.\n",
    "\n",
    "<details>\n",
    "<summary markdown=\"span\">Решение </summary>\n",
    "<div>\n",
    "Если $\\boldsymbol B = [\\boldsymbol b_1 \\ldots \\boldsymbol b_p]$, то\n",
    "\n",
    "$$\n",
    "\\boldsymbol A \\boldsymbol B = [\\boldsymbol A\\boldsymbol b_1 \\ldots \\boldsymbol A\\boldsymbol b_p], \\quad (\\boldsymbol {AB})^\\top = \\begin{pmatrix}\n",
    "        \\boldsymbol b_1^\\top \\boldsymbol A^\\top \\\\\n",
    "        \\boldsymbol b_2^\\top \\boldsymbol A^\\top \\\\\n",
    "        \\vdots\\\\\n",
    "        \\boldsymbol b_p^\\top \\boldsymbol A^\\top \n",
    "    \\end{pmatrix} = \\boldsymbol B^\\top \\boldsymbol A^\\top.\n",
    "$$ \n",
    "\n",
    "Теперь разберёмся со следом. Пусть $\\boldsymbol A \\in\\mathbb R^{m\\times n}$, $\\boldsymbol B \\in\\mathbb R^{n\\times m}$, тогда\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}(\\boldsymbol{AB})  = \\sum\\limits_{i=1}^m \\sum\\limits_{j=1}^n A_{ij} B_{ji}.\n",
    "$$\n",
    "\n",
    "Меняя порядок суммирования находим, что это же выражение равно\n",
    "\n",
    "$$\n",
    " \\sum\\limits_{j=1}^n \\sum\\limits_{i=1}^m B_{ji}A_{ij} = \\mathrm{tr}(\\boldsymbol{BA}).\n",
    "$$\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "\n",
    "Несколько дополнительных свойств для квадратных матриц:\n",
    "\n",
    "* $\\boldsymbol{AI} = \\boldsymbol{IA} = \\boldsymbol A$;\n",
    "\n",
    "* если $\\boldsymbol P$ — матрица перестановки, то матрица $\\boldsymbol{PA}$ получается из матрицы $\\boldsymbol A$ перестановкой строк, а матрица $\\boldsymbol{AP}$ — перестановкой столбцов;\n",
    "\n",
    "* произведение двух верхних (нижних) треугольных матриц тоже верхняя (нижняя) треугольная матрица;\n",
    "\n",
    "* если матрица $\\boldsymbol Q$ ортогональна, то $\\boldsymbol{QQ}^\\top = \\boldsymbol Q^\\top \\boldsymbol Q = \\boldsymbol I$.\n",
    "\n",
    "А вот коммутировать квадратные матрицы не обязаны: в общем случае $\\boldsymbol{AB} \\ne \\boldsymbol{BA}$ (поищите контрпример среди матриц размера $2\\times 2$).\n",
    "\n",
    "**Вопрос на подумать**. Верно ли, что произведение двух симметричных матриц симметрично?\n",
    "\n",
    "<details>\n",
    "<summary markdown=\"span\">Ответ </summary>\n",
    "<div>\n",
    "  Если $\\boldsymbol A^\\top = \\boldsymbol A$, $\\boldsymbol B^\\top = \\boldsymbol B$, то по свойству транспонирования произведения получаем $(\\boldsymbol{AB})^\\top = \\boldsymbol B^\\top \\boldsymbol A^\\top = \\boldsymbol B\\boldsymbol A$, что не обязательно равно $\\boldsymbol A\\boldsymbol B$. Нетрудно подобрать контрпример среди матриц размера $2\\times 2$:\n",
    "\n",
    "  $$\n",
    "    \\begin{pmatrix}\n",
    "        1 & 0 \\\\\n",
    "        0 & 2 \\\\\n",
    "    \\end{pmatrix}\n",
    "    \\cdot\n",
    "    \\begin{pmatrix}\n",
    "        1 & -1 \\\\\n",
    "        -1 & 1 \\\\\n",
    "    \\end{pmatrix}\n",
    "    =\n",
    "    \\begin{pmatrix}\n",
    "        1 & -1 \\\\\n",
    "        -2 & 2 \\\\\n",
    "    \\end{pmatrix}.\n",
    "  $$\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "Квадратные матрицы можно возводить в натуральную степень так же, как и обычные числа. Вот, например, индуктивное определение степени матрицы:\n",
    "\n",
    "$$\n",
    "    \\boldsymbol A^n = \\boldsymbol A^{n-1} \\boldsymbol A, \\quad n\\in\\mathbb N,\\quad \\boldsymbol A^0 = \\boldsymbol I.\n",
    "$$\n",
    "\n",
    "Справедливы равенства \n",
    "\n",
    "$$\n",
    "    \\boldsymbol A^{m+n} = \\boldsymbol A^m\\boldsymbol A^n,\\quad \\boldsymbol A^{mn} = \\boldsymbol (\\boldsymbol A^m)^n, \\quad m, n \\in \\mathbb N.\n",
    "$$\n",
    "\n",
    "Можно даже пойти дальше, и определить взятие экспоненты или синуса от матрицы через ряд Тейлора:\n",
    "\n",
    "$$\n",
    "    \\exp(\\boldsymbol A) = \\sum\\limits_{n=0}^\\infty \\frac{\\boldsymbol A^n}{n!},\\quad\n",
    "    \\sin(\\boldsymbol A) = \\sum\\limits_{n=0}^\\infty (-1)^n\\frac{\\boldsymbol A^{2n+1}}{(2n+1)!}\n",
    "$$\n",
    "\n",
    "(можно доказать, что эти ряды сходятся).\n",
    "\n",
    "**Замечание**. В машинном (и особенно глубинном) обучении часто берут числовую функцию от матрицы, например $\\log(\\boldsymbol A)$, $\\tanh(\\boldsymbol A)$ или $\\sigma(\\boldsymbol A)$, где $\\sigma(x) = \\frac 1{1+e^{-x}}$ — сигмоида. Такая запись почти наверное подразумевает поэлементное применение функции к каждой ячейке матрицы $\\boldsymbol A$, а вовсе не матричные ряды! Тем более что матрицы в машинном обучении чаще всего прямоугольные, а для них нельзя определить ни степень, ни ряд.\n",
    "\n",
    "## Одноранговые матрицы\n",
    "\n",
    "**Одноранговая матрица** задаётся произведением столбца на строку: $\\boldsymbol A = \\boldsymbol {uv}^\\top$, $\\boldsymbol u \\in \\mathbb R^m$, $\\boldsymbol v \\in \\mathbb R^n$. Элементы этой прямоугольной матрицы равны $A_{ij} = u_i v_j$, а все её строки/столбцы пропорциональны. Отметим также, что если $m=n$, то \n",
    "\n",
    "$$\n",
    "    \\mathrm{tr}(\\boldsymbol {uv}^\\top) = \\sum\\limits_{i=1}^n u_i v_i = \\boldsymbol v^\\top \\boldsymbol u = \\mathrm{tr}(\\boldsymbol v^\\top \\boldsymbol u).\n",
    "$$\n",
    "\n",
    "Произведение матриц $\\boldsymbol A \\in\\mathbb R^{m\\times n}$ и $\\boldsymbol B \\in\\mathbb R^{n\\times p}$ можно записать в виде суммы одноранговых матриц:\n",
    "\n",
    "$$\n",
    "    \\boldsymbol {AB} = [\\boldsymbol a_1, \\ldots, \\boldsymbol a_n] \\begin{pmatrix}\n",
    "        \\boldsymbol b_1^\\top \\\\\n",
    "        \\boldsymbol b_2^\\top \\\\\n",
    "        \\vdots\\\\\n",
    "        \\boldsymbol b_n^\\top \\\\\n",
    "    \\end{pmatrix} = \n",
    "    \\sum\\limits_{j=1}^n \\boldsymbol a_j  \\boldsymbol b_j^\\top.\n",
    "$$\n",
    "\n",
    "В англоязычной литературе умножение столбца на строку $\\boldsymbol {uv}^\\top$ называется **outer product** («внешнее произведение»), по аналогии со скалярным произведением  $\\boldsymbol u^\\top\\boldsymbol v$, называемым **inner product** («внутреннее произведение»).\n",
    "\n",
    "## Блочные матрицы\n",
    "\n",
    "**Блочная матрица** имеет вид\n",
    "\n",
    "$$\n",
    "        \\begin{pmatrix}\n",
    "        \\boldsymbol A & \\boldsymbol B \\\\\n",
    "        \\boldsymbol C & \\boldsymbol D \n",
    "        \\end{pmatrix} =\n",
    "        \\left(\\begin{array}{@{}c|c@{}}\n",
    "        \\begin{matrix}\n",
    "            a_{11} & \\dots & a_{1m} \\\\\n",
    "            \\vdots & \\ddots & \\vdots \\\\\n",
    "            a_{k1} & \\dots & a_{km}\n",
    "        \\end{matrix}\n",
    "        &\n",
    "        \\begin{matrix}\n",
    "            b_{11} & \\dots & b_{1n} \\\\\n",
    "            \\vdots & \\ddots & \\vdots \\\\\n",
    "            b_{k1} & \\dots & b_{kn}\n",
    "        \\end{matrix} \\\\\n",
    "        \\hline\n",
    "        \\begin{matrix}\n",
    "            c_{11} & \\dots & c_{1m} \\\\\n",
    "            \\vdots & \\ddots & \\vdots \\\\\n",
    "            c_{\\ell 1} & \\dots & c_{\\ell m}\n",
    "        \\end{matrix} \n",
    "        &\n",
    "        \\begin{matrix}\n",
    "            d_{11} & \\dots & d_{1n} \\\\\n",
    "            \\vdots & \\ddots & \\vdots \\\\\n",
    "            d_{\\ell 1} & \\dots & d_{\\ell n}\n",
    "        \\end{matrix}\n",
    "        \\end{array}\\right).\n",
    "$$\n",
    "\n",
    "**Контрольный вопрос**. Каков будет результат транспонирования блочной матрицы\n",
    "\n",
    "$$\n",
    "    \\begin{pmatrix}\n",
    "        \\boldsymbol A & \\boldsymbol B \\\\\n",
    "        \\boldsymbol C & \\boldsymbol D \n",
    "    \\end{pmatrix},\n",
    "$$\n",
    "\n",
    "где $\\boldsymbol A \\in \\mathbb R^{k\\times m}$, $\\boldsymbol B \\in \\mathbb R^{k\\times n}$, $\\boldsymbol C \\in \\mathbb R^{\\ell\\times m}$, $\\boldsymbol D \\in \\mathbb R^{\\ell\\times n}$?\n",
    "\n",
    "<details>\n",
    " <summary markdown=\"span\">Ответ </summary>\n",
    "<div>\n",
    "Блоки $\\boldsymbol B$ и $\\boldsymbol C$ надо переставить, и не забыть ещё транспонировать каждый блок:\n",
    "\n",
    "$$\n",
    "    \\begin{pmatrix}\n",
    "        \\boldsymbol A^\\top & \\boldsymbol C^\\top \\\\\n",
    "        \\boldsymbol B^\\top & \\boldsymbol D^\\top \n",
    "    \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "Блочные матрицы могут иметь произвольное число матричных блоков по каждому измерению. Интересно, что перемножать блочные матрицы можно по тем же правилам, что и матрицы из чисел, например\n",
    "\n",
    "$$\n",
    "    \\begin{pmatrix}\n",
    "        \\boldsymbol A_{11} & \\boldsymbol A_{12} \\\\\n",
    "        \\boldsymbol A_{21} & \\boldsymbol A_{22} \n",
    "    \\end{pmatrix}\n",
    "    \\begin{pmatrix}\n",
    "        \\boldsymbol B_1 \\\\\n",
    "        \\boldsymbol B_2 \n",
    "    \\end{pmatrix} = \n",
    "    \\begin{pmatrix}\n",
    "        \\boldsymbol A_{11} \\boldsymbol B_1 + \\boldsymbol A_{12} \\boldsymbol B_2 \\\\\n",
    "        \\boldsymbol A_{21} \\boldsymbol B_1 + \\boldsymbol A_{22} \\boldsymbol B_2\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "(при условии, что размеры матриц позволяют корректно произвести все матричные умножения).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
