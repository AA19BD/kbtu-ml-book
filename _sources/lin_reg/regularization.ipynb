{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "If the columns of the feature matrix $\\boldsymbol X$ are highly correlated, the weights of linear regression could be unstable and excessively large. To fight this, a **regularization** term is added to the MSE loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "\n",
    "To penalize weights that are too large in magnitude, add a multiple of \n",
    "\n",
    "$$\n",
    "\\Vert \\boldsymbol w \\Vert_2^2 = \\boldsymbol w^\\top \\boldsymbol w = \\sum\\limits_{j=1}^d w_j^2\n",
    "$$ \n",
    "\n",
    "to RSS; the new loss function is\n",
    "\n",
    "$$\n",
    "    \\mathcal L(\\boldsymbol w) = \\Vert \\boldsymbol y - \\boldsymbol{Xw}\\Vert_2^2 + C\\Vert \\boldsymbol w \\Vert_2^2, \\quad C>0.\n",
    "$$\n",
    "\n",
    "To find the optimal weights, one need to solve the equation $\\nabla_{\\boldsymbol w} \\mathcal L(\\boldsymbol w) = \\boldsymbol 0$.\n",
    "\n",
    "Since $\\nabla_{\\boldsymbol w}(\\Vert \\boldsymbol w \\Vert_2^2) = 2\\boldsymbol w$, we receive one additional term to {eq}`lin-reg-grad`:\n",
    "\n",
    "$$\n",
    "    \\boldsymbol X^\\top \\boldsymbol X\\boldsymbol w - \\boldsymbol X^\\top \\boldsymbol y + C\\boldsymbol w = \\boldsymbol 0.\n",
    "$$\n",
    "\n",
    "From here we obtain\n",
    "\n",
    "$$\n",
    "    \\widehat{\\boldsymbol w} = (\\boldsymbol X^\\top \\boldsymbol X + C\\boldsymbol I)^{-1} \\boldsymbol X^\\top \\boldsymbol y.\n",
    "$$\n",
    "\n",
    "Note that this is equal to {eq}`lin-reg-solution` if $C=0$. Ridge regression also allows to avoid inverting of a singular matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again take the Boston dataset, and try ridge regression on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: 36.66359752181638\n",
      "coefficients: [-1.18324104e-01  4.80549483e-02 -1.78537994e-02  2.70383610e+00\n",
      " -1.14024737e+01  3.70139930e+00 -2.75076805e-03 -1.38242357e+00\n",
      "  2.71825248e-01 -1.33051618e-02 -8.55687104e-01 -5.62037458e-01]\n",
      "r-score: 0.73233360340788\n",
      "MSE: 22.59627839822696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "boston = pd.read_csv(\"../ISLP_datsets/Boston.csv\")\n",
    "\n",
    "target = boston['medv']\n",
    "train = boston.drop(['medv', \"Unnamed: 0\"], axis=1)\n",
    "ridge = Ridge()\n",
    "ridge.fit(train, target)\n",
    "print(\"intercept:\", ridge.intercept_)\n",
    "print(\"coefficients:\", ridge.coef_)\n",
    "print(\"r-score:\", ridge.score(train, target))\n",
    "print(\"MSE:\", np.mean((ridge.predict(train) - target) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso regression\n",
    "\n",
    "The only difference with ridge regression is that $L^1$-norm is used for the regularization term:\n",
    "\n",
    "$$\n",
    "    \\Vert \\boldsymbol w \\Vert_1 = \\sum\\limits_{j=1}^d \\vert w_j \\vert.\n",
    "$$\n",
    "\n",
    "Accordingly, the loss function switches to\n",
    "\n",
    "$$\n",
    "    \\mathcal L(\\boldsymbol w) = \\Vert \\boldsymbol y - \\boldsymbol{Xw}\\Vert_2^2 + C\\Vert \\boldsymbol w \\Vert_1, \\quad C>0.\n",
    "$$\n",
    "\n",
    "Important property of Lasso regression is feature selection. Usually some coorditates of the optimal vector of weights $\\widehat {\\boldsymbol w}$ turn out to be zero. A famous picture from {cite}`pml1Book`:\n",
    "\n",
    "```{figure} images/Figure_11.8.png\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how Lasso regression works on Boston dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: 44.98510958345055\n",
      "coefficients: [-0.07490856  0.05004565 -0.          0.         -0.          0.83244563\n",
      "  0.02266242 -0.66083402  0.25070221 -0.01592473 -0.70206981 -0.7874801 ]\n",
      "r-score: 0.6745576822708007\n",
      "MSE: 27.47369601713281\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso()\n",
    "lasso.fit(train, target)\n",
    "print(\"intercept:\", lasso.intercept_)\n",
    "print(\"coefficients:\", lasso.coef_)\n",
    "print(\"r-score:\", lasso.score(train, target))\n",
    "print(\"MSE:\", np.mean((lasso.predict(train) - target) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three coeffitients vanished, namely `indus`, `chas`, `nox`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['indus', 'chas', 'nox'], dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.columns[3:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net\n",
    "\n",
    "**Elasitc net** is a hybrid between lasso and ridge regression. This corresponds to minimizing the following objective:\n",
    "\n",
    "$$\n",
    "    \\mathcal L(\\boldsymbol w) = \\Vert \\boldsymbol y - \\boldsymbol{Xw}\\Vert_2^2 + C_1\\Vert \\boldsymbol w \\Vert_1 + C_2\\Vert \\boldsymbol w \\Vert_2^2, \\quad C_1, C_2>0.\n",
    "$$\n",
    "\n",
    "Applied to Boston dataset, it zeroes two coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: 45.68409537995804\n",
      "coefficients: [-0.09222698  0.05335925 -0.02011639  0.         -0.          0.89077605\n",
      "  0.02193951 -0.7586982   0.28439842 -0.01686424 -0.72640381 -0.77837394]\n",
      "r-score: 0.679766219677128\n",
      "MSE: 27.033993601067873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "en = ElasticNet()\n",
    "en.fit(train, target)\n",
    "print(\"intercept:\", en.intercept_)\n",
    "print(\"coefficients:\", en.coef_)\n",
    "print(\"r-score:\", en.score(train, target))\n",
    "print(\"MSE:\", np.mean((en.predict(train) - target) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Prove that the matrix $\\boldsymbol X^\\top \\boldsymbol X + C\\boldsymbol I$ is invertible for all $C >0$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
