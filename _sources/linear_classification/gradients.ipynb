{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient optimization\n",
    "\n",
    "To find the optimal weights of the logistic regression, we can use {prf:ref}`gradient descent <GD>` algorithm. To apply this algorithm, one need to calculate the gradient of the loss function.\n",
    "\n",
    "## Binary logistic regression\n",
    "\n",
    "Multiply the loss function {eq}`bin-log-reg-loss` by $n$:\n",
    "\n",
    "$$\n",
    "\\mathcal L(\\boldsymbol w) = \n",
    "-\\sum\\limits_{i=1}^n \\big(y_i \\log(\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)) + (1- y_i)\\log(1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w))\\big).\n",
    "$$\n",
    "\n",
    "To find $\\nabla \\mathcal L(\\boldsymbol w)$ observe that\n",
    "\n",
    "$$\n",
    "   \\nabla \\log(\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)) = \\frac {\\nabla \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}{\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)} = \n",
    "   \\frac{\\sigma'(\\boldsymbol x_i^\\top \\boldsymbol w) \\nabla(\\boldsymbol x_i^\\top \\boldsymbol w)}{{\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}}.\n",
    "$$\n",
    "\n",
    "Also,\n",
    "\n",
    "$$\n",
    "   \\nabla \\log(1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)) = -\\frac {\\nabla  \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}{1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)} = \n",
    "   \\frac{\\sigma'(\\boldsymbol x_i^\\top \\boldsymbol w) \\nabla(\\boldsymbol x_i^\\top \\boldsymbol w)}{{1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}}.\n",
    "$$\n",
    "\n",
    "**Q**. What is $\\nabla(\\boldsymbol x_i^\\top \\boldsymbol w)$?\n",
    "\n",
    "Putting it altogeter, we get\n",
    "\n",
    "$$\n",
    "   \\nabla \\mathcal L(\\boldsymbol w) = -\\sum\\limits_{i=1}^n \\big(y_i(1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w))\\boldsymbol x_i - (1-y_i)\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)\\boldsymbol x_i\\big) = \\sum\\limits_{i=1}^n (\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w) - y_i)\\boldsymbol x_i.\n",
    "$$\n",
    "\n",
    "**Q**. How to write $\\nabla \\mathcal L(\\boldsymbol w)$ as a product of a matrix and a vector, avoiding the explicit summation?\n",
    "\n",
    "**Q**. What is hessian $\\nabla^2 L(\\boldsymbol w)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial logistic regression\n",
    "\n",
    "Recall that the loss function in this case is\n",
    "\n",
    "$$\n",
    "    \\begin{multline*}\n",
    "    \\mathcal L(\\boldsymbol W) = \\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^K  y_{ik} \\bigg(\\boldsymbol x_i^\\top\\boldsymbol w_{k} -\\log\\Big(\\sum\\limits_{k=1}^K \\exp(\\boldsymbol x_i^\\top\\boldsymbol w_{k})\\Big)\\bigg) = \\\\\n",
    "    =\n",
    "    \\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^K  y_{ik} \\bigg(\\sum\\limits_{j=1}^d x_{ij} w_{jk} -\\log\\Big(\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)\\Big)\\bigg)\n",
    "    \\end{multline*}\n",
    "$$\n",
    "\n",
    "Observe that\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial}{\\partial w_{pq}} (x_{ij} w_{jk}) = x_{ij} \\delta_{pj} \\delta_{qk},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_{pq}}\\bigg(\\log\\Big(\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)\\Big)\\bigg) = \\frac{\\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)}{\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)} x_{ip} \\delta_{qk}\n",
    "$$\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$\n",
    "    \\frac{\\partial \\mathcal L}{\\partial w_{pq}} = \\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^K y_{ik}\\bigg(\\sum\\limits_{j=1}^d  \\bigg(  x_{ij} \\delta_{pj} \\delta_{qk} - \\frac{\\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)}{\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)} x_{ip} \\delta_{qk}\\bigg)\\bigg)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
