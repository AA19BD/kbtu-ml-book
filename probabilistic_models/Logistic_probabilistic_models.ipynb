{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939f2946",
   "metadata": {},
   "source": [
    "# Probabilistic models for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38623aa1",
   "metadata": {},
   "source": [
    "Bayesian Logistic Regression is an extension of traditional logistic regression that incorporates a Bayesian framework. In traditional logistic regression, you estimate the model parameters using maximum likelihood estimation (MLE), which provides point estimates for the coefficients. In Bayesian Logistic Regression, you go a step further by treating these coefficients as random variables with associated probability distributions, allowing you to capture uncertainty in model parameters and make probabilistic predictions. Here's an explanation of key concepts in Bayesian Logistic Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf7ec53",
   "metadata": {},
   "source": [
    "1. <h4>Logistic Regression.</h4>\n",
    "\n",
    "Logistic Regression is a binary classification algorithm used to model the probability of a binary outcome (0 or 1) as a function of one or more predictor variables. The logistic function (sigmoid) is used to transform a linear combination of predictor variables into a probability score between 0 and 1.\n",
    "\n",
    "The logistic regression model can be written as:\n",
    "\n",
    "$$\n",
    "P(Y=1 \\mid X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p)}}\n",
    "$$\n",
    "\n",
    "Where: \n",
    "* $ P(Y=1 \\mid X)$ is the probability of the binary outcome being 1 given the predictor variables X;\n",
    "\n",
    "* ${\\beta_1}, \\beta_2\\, \\beta_3\\$, ... are the coefficients to be estimated.\n",
    "\n",
    "2. <h4>Bayesian Framework.</h4>\n",
    "\n",
    "In Bayesian statistics, we view model parameters as random variables with associated probability distributions. Instead of finding a single point estimate for these parameters, we estimate their full probability distributions, which allows us to quantify uncertainty.\n",
    "\n",
    "3. <h4>Prior Distributions:</h4>\n",
    "\n",
    "In Bayesian Logistic Regression, you specify prior probability distributions for the model parameters (${\\beta_1}, \\beta_2\\, ... ,\\beta_p\\$). These prior distributions represent your beliefs or prior knowledge about the parameters before observing any data. You can choose informative priors based on domain knowledge or non-informative priors for minimal prior influence.\n",
    "\n",
    "4. <h4>Likelihood.</h4>\n",
    "\n",
    "The likelihood function quantifies the probability of observing the data given the model and its parameters. For logistic regression, the likelihood is often assumed to follow a Bernoulli distribution, representing the probability of observing each binary outcome.\n",
    "\n",
    "5. <h4>Posterior Distribution</h4>\n",
    "\n",
    "Bayes' theorem is used to update the prior beliefs with the likelihood of the data to obtain the posterior distribution of the parameters. The posterior distribution represents your updated beliefs about the parameters after considering the data.\n",
    "\n",
    "6. <h4>Sampling or Numerical Methods</h4>\n",
    "\n",
    "Estimating the posterior distribution analytically can be challenging, so Bayesian Logistic Regression often involves using sampling methods like Markov Chain Monte Carlo (MCMC) or variational inference. These methods draw samples from the posterior distribution, allowing you to estimate the parameter distributions.\n",
    "\n",
    "7. <h4>Predictive Probabilities.</h4>\n",
    "\n",
    "Once you have the posterior distribution of the parameters, you can make probabilistic predictions. Instead of a single predicted probability, you obtain a distribution of probabilities for each observation, reflecting the uncertainty in the model.\n",
    "\n",
    "8. <h4>Model Evaluation</h4>\n",
    "\n",
    "Bayesian Logistic Regression allows for more informative model evaluation, including credible intervals for model parameters and predictive intervals for individual predictions.\n",
    "\n",
    "\n",
    "In summary, Bayesian Logistic Regression extends logistic regression by incorporating a probabilistic framework. It provides a more comprehensive understanding of model uncertainty and is particularly useful when you need to quantify uncertainty in model parameters and predictions or when you want to incorporate prior knowledge into your modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f9f1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.457956\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  100\n",
      "Model:                          Logit   Df Residuals:                       97\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Sat, 23 Sep 2023   Pseudo R-squ.:                  0.3345\n",
      "Time:                        20:30:08   Log-Likelihood:                -45.796\n",
      "converged:                       True   LL-Null:                       -68.814\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.008e-10\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.1969      0.259     -0.759      0.448      -0.705       0.311\n",
      "x1             1.6565      0.350      4.738      0.000       0.971       2.342\n",
      "x2            -0.9378      0.289     -3.245      0.001      -1.504      -0.371\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(100, 2)\n",
    "true_coeffs = np.array([1.5, -0.8])\n",
    "linear_combination = np.dot(X, true_coeffs)\n",
    "p = 1 / (1 + np.exp(-linear_combination))\n",
    "y = np.random.binomial(1, p, size=100)\n",
    "\n",
    "# Add a constant term to the predictor matrix (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Create and fit a Bayesian Logistic Regression model\n",
    "logit_model = sm.Logit(y, X)\n",
    "result = logit_model.fit()\n",
    "\n",
    "# Print model summary\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7270b828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Probabilities:\n",
      "[0.35854762 0.23048352 0.01952909 0.56101621 0.79233815 0.24157171\n",
      " 0.75064029 0.19324655 0.20129829 0.28436935]\n",
      "\n",
      "Predicted Labels:\n",
      "[0 0 0 1 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Generate new data for prediction (similar format as the training data)\n",
    "new_data = np.random.randn(10, 2)\n",
    "new_data = sm.add_constant(new_data)  # Add the intercept term\n",
    "\n",
    "# Use the trained model to make predictions on new data\n",
    "predicted_probabilities = result.predict(new_data)\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1) based on a threshold (e.g., 0.5)\n",
    "threshold = 0.5\n",
    "predicted_labels = (predicted_probabilities > threshold).astype(int)\n",
    "\n",
    "# Print the predicted probabilities and labels\n",
    "print(\"Predicted Probabilities:\")\n",
    "print(predicted_probabilities)\n",
    "\n",
    "print(\"\\nPredicted Labels:\")\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7605ad96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce8863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
