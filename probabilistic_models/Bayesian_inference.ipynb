{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3064898-d081-4dec-85ae-ec5c8391a096",
   "metadata": {},
   "source": [
    "# Bayesian inference\n",
    " \n",
    "\n",
    "Bayesian inference is a statistical method for updating and revising probabilities based on new evidence or data. It is named after Thomas Bayes, an 18th-century statistician and theologian who developed the foundational ideas behind this approach.\n",
    "\n",
    "\n",
    "At its core, Bayesian inference is a way to quantify uncertainty and make probabilistic predictions or decisions in the presence of incomplete or uncertain information. It revolves around Bayes' theorem, which describes how to update the probability of a hypothesis (an uncertain statement or proposition) based on new evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9874a31-656b-4975-b276-e78f73877ff3",
   "metadata": {},
   "source": [
    "<h6>In machine learning</h6>\n",
    "Bayesian inference is a fundamental concept that involves using probability theory to make predictions, update beliefs, and model uncertainty. The basic concept of Bayesian inference in the context of machine learning can be broken down into several key components:\n",
    "\n",
    "1. <h6>Bayesian Framework:</h6>\n",
    "   Bayesian inference is built on the Bayesian probability framework, which views probabilities as measures of uncertainty or degrees of belief. Instead of thinking of model parameters or predictions as fixed values, Bayesian methods treat them as random variables with probability distributions. This allows us to express and update our uncertainty about these variables.\n",
    "\n",
    "3. <h6>Prior Beliefs:</h6> \n",
    "    Bayesian inference starts with prior beliefs or prior probabilities. These beliefs represent your initial assumptions or knowledge about the parameters of your model before observing any data. Priors can be chosen to reflect what you believe about the parameters based on domain knowledge or historical data.\n",
    "\n",
    "4. <h6>Likelihood:</h6>\n",
    "   The likelihood function quantifies the probability of observing the data given a specific set of model parameters. It represents the likelihood of the observed data under different conditions or parameter settings. In many machine learning models, the likelihood is assumed to follow a particular probability distribution (e.g., Gaussian, Bernoulli) depending on the problem.\n",
    "\n",
    "6. <h6>Posterior Distribution:</h6>\n",
    "   The goal of Bayesian inference is to compute the posterior distribution of the model parameters. The posterior is a probability distribution that combines the prior beliefs and the likelihood of the data. It represents your updated beliefs about the parameters after observing the data. Mathematically, the posterior is computed using Bayes' theorem:\n",
    "\n",
    "$$\n",
    " P(\\text{parameters} | \\text{data}) =  \\frac{P(\\text{data} | \\text{parameters}) \\cdot P(\\text{parameters})} {P(\\text{data})}\n",
    "$$\n",
    "Where: \n",
    "\n",
    "* P(parameters|data) is the posterior distribution;\n",
    "* P(data|parameters) is the likelihood;\n",
    "* P(parameters) is the prior distribution;\n",
    "* P(data) is the marginal likelihood (a normalizing constant).\n",
    "    \n",
    "\n",
    "8. <h6>Posterior Inference:</h6>\n",
    "   Once you have the posterior distribution, you can perform various inference tasks. For example, you can calculate the mean, median, or mode of the posterior to obtain point estimates of the model parameters. You can also compute credible intervals to quantify uncertainty. Bayesian inference provides a full distribution of possible parameter values, allowing for more comprehensive uncertainty quantification.\n",
    "\n",
    "10. <h6>Bayesian Decision Making:</h6>\n",
    "    In machine learning, Bayesian inference can be used not only for parameter estimation but also for decision making. You can make decisions by choosing actions that minimize expected loss with respect to the posterior distribution. This is particularly useful in settings where you need to make decisions under uncertainty.\n",
    "\n",
    "12. <h6>Sequential Learning and Updating:</h6>\n",
    "    Bayesian inference naturally supports sequential learning, where you update your beliefs and predictions as new data becomes available. After observing new data, you can use the posterior from the previous step as the prior for the next step, allowing your model to adapt to changing conditions.\n",
    "\n",
    "In summary, Bayesian inference in machine learning provides a principled framework for modeling uncertainty, updating beliefs, making decisions, and performing various inference tasks. It allows practitioners to incorporate prior knowledge, adapt to new data, and quantify uncertainty in a coherent and probabilistic manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b3c95-3b97-4f41-b344-f181ebdb689f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d7a51-f557-4ac9-82ca-2cc2ee700414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde10a7-3ad5-49a2-b5bb-d16fe289d317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b50d0a-8a93-4ce3-9e78-063d95932ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
